adapter_path: adapters
batch_size: 4
data: data
fine_tune_type: lora
grad_checkpoint: true
iters: 500
learning_rate: 1.0e-05
lora_parameters:
  dropout: 0.05
  keys:
  - self_attn.q_proj
  - self_attn.v_proj
  rank: 8
  scale: 20.0
max_seq_length: 2048
model: meta-llama/llama-3.2-3B-Instruct
num_layers: 16
save_every: 100
seed: 0
steps_per_eval: 100
steps_per_report: 10
test: false
test_batches: 100
train: true
val_batches: 25
